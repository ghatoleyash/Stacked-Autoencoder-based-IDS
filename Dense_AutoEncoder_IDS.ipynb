{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "15o0bNY4mBMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC-mPuyRmD4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('kddcup99_csv.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1xCvUEbmGdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check if any null values are exisiting in the data, also getting the number of categorical as well as numerical columns \n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkEIjLAhmID6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking for duplicate rows\n",
        "duplicate_rows=df[df.duplicated()]\n",
        "print(duplicate_rows)\n",
        "df.drop_duplicates(keep=False,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w49dI6ETmJkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Protocol_type, Flag, Urgent were the categorical columns\n",
        "print(df['protocol_type'].unique())\n",
        "print(df['flag'].unique())\n",
        "print(df['urgent'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-xjycFBmLrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_copy=df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj6AnP4DmPE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onehotencode=OneHotEncoder()\n",
        "labelencode=LabelEncoder()\n",
        "\n",
        "#Creating the labelencode for label class 0:- attack, 1:- normal\n",
        "df_copy.loc[df_copy['label']!='normal','label']='attack'\n",
        "df_copy['label']=labelencode.fit_transform(df_copy['label'])\n",
        "\n",
        "\n",
        "#Creating OnehotEncoding for the categorical columns\n",
        "X_protocol_type = onehotencode.fit_transform(df_copy.protocol_type.values.reshape(-1,1)).toarray()\n",
        "X_service = onehotencode.fit_transform(df_copy.service.values.reshape(-1,1)).toarray()\n",
        "X_flag = onehotencode.fit_transform(df_copy.flag.values.reshape(-1,1)).toarray()\n",
        " \n",
        "#Creating dataframe for the OnehotEncoded columns \n",
        "df_protocol = pd.DataFrame(X_protocol_type, columns =df_copy['protocol_type'].unique()) \n",
        "df_service = pd.DataFrame(X_service, columns = df_copy['service'].unique()) \n",
        "df_flag = pd.DataFrame(X_flag, columns = df_copy['flag'].unique()) \n",
        "\n",
        "\n",
        "\n",
        "df_onehot_encoded=df_copy\n",
        "lst_proto=df_protocol.columns\n",
        "lst_service=df_service.columns\n",
        "lst_flag=df_flag.columns\n",
        "\n",
        "\n",
        "df_onehot_encoded= df_onehot_encoded.drop(['protocol_type','flag','service'], axis=1)\n",
        "\n",
        "#Assigning remaining numerical columns to a single dataframe\n",
        "for i in range(0,len(lst_proto)):\n",
        "  df_onehot_encoded[lst_proto[i]]=df_protocol[lst_proto[i]].values\n",
        "print(df_onehot_encoded.isna().sum().sum())\n",
        "\n",
        "for i in range(0,len(lst_service)):\n",
        "  df_onehot_encoded[lst_service[i]]=df_service[lst_service[i]].values\n",
        "\n",
        "for i in range(0,len(lst_flag)):\n",
        "  df_onehot_encoded[lst_flag[i]]=df_flag[lst_flag[i]].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9otAGhCDmQwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test Set is 20% of the total data\n",
        "#Cross Validation set is 20% of the training data \n",
        "\n",
        "data_split_pct=0.2\n",
        "#Test set\n",
        "df_train_onehot_encoded,df_test_onehot_encoded=train_test_split(df_onehot_encoded,test_size=data_split_pct)\n",
        "#cross validation set\n",
        "df_train_onehot_encoded,df_cv_onehot_encoded=train_test_split(df_train_onehot_encoded,test_size=data_split_pct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqZ5YjY7mRxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the data on single class\n",
        "df_train_onehot_encoded_normal=df_train_onehot_encoded.loc[df_train_onehot_encoded['label']!=0]\n",
        "df_train_onehot_encoded_normal_wo_label=df_train_onehot_encoded_normal.drop(['label'],axis=1)\n",
        "df_test_onehot_encoded_wo_label=df_test_onehot_encoded.drop(['label'],axis=1)\n",
        "df_cv_onehot_encoded_wo_label=df_cv_onehot_encoded.drop(['label'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAVr93ObmRzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Standardizing the values \n",
        "scaler = StandardScaler().fit(df_train_onehot_encoded_normal_wo_label)\n",
        "df_train_sel_col_wo_label_rescaled = scaler.transform(df_train_onehot_encoded_normal_wo_label)\n",
        "df_test_sel_col_wo_label_rescaled = scaler.transform(df_test_onehot_encoded_wo_label)\n",
        "df_cv_sel_col_wo_label_rescaled = scaler.transform(df_cv_onehot_encoded_wo_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOdqe_JGmR13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AutoEncoder Architecture\n",
        "\n",
        "epoch=500\n",
        "batch_size=2000\n",
        "input_dim=df_train_sel_col_wo_label_rescaled.shape[1]\n",
        "encoding_dim=64\n",
        "hidden_dim=int(encoding_dim/2)\n",
        "learning_rate=1e-3\n",
        "\n",
        "\n",
        "input_layer=Input(shape=(input_dim,))\n",
        "encoder = Dense(encoding_dim, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
        "encoder=BatchNormalization()(encoder)\n",
        "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)#encoded features\n",
        "encoder=BatchNormalization()(encoder)\n",
        "decoder = Dense(encoding_dim, activation=\"relu\")(encoder)\n",
        "decoder=BatchNormalization()(decoder)\n",
        "decoder = Dense(input_dim, activation=\"linear\")(decoder)#decoded features\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "initial_weights=autoencoder.get_weights()\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiGEZJ7bmR44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fitting the model using error metric as mean squared error and adam gradient optimizer\n",
        "autoencoder.compile(metrics=['mse'],\n",
        "                    loss='mean_squared_error',\n",
        "                    optimizer='adam')\n",
        "cp = ModelCheckpoint(filepath=\"autoencoder_classifier.h5\",\n",
        "                               save_best_only=True,\n",
        "                               verbose=0)\n",
        "tb = TensorBoard(log_dir='./logs',\n",
        "                histogram_freq=0,\n",
        "                write_graph=True,\n",
        "                write_images=True)\n",
        "history = autoencoder.fit(df_train_sel_col_wo_label_rescaled, df_train_sel_col_wo_label_rescaled,\n",
        "                    epochs=epoch,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    validation_data=(df_cv_sel_col_wo_label_rescaled, df_cv_sel_col_wo_label_rescaled),\n",
        "                    verbose=1,\n",
        "                    callbacks=[cp, tb]).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGXn_jcCmR1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EVALUATING CROSS VALIDATION SET\n",
        "valid_x_predictions = autoencoder.predict(df_cv_sel_col_wo_label_rescaled)\n",
        "\n",
        "#Calculating mean squared error between predictions and cros vaildation values\n",
        "mse = np.mean(np.power(df_cv_sel_col_wo_label_rescaled - valid_x_predictions, 2), axis=1)\n",
        "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "                        'True_class': df_cv_onehot_encoded['label']})\n",
        "error_df = error_df.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07uRRGDlmQyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating precision and recall value at various threshold values\n",
        "precision=[]\n",
        "recall=[]\n",
        "threshold_cv=[]\n",
        "i=0\n",
        "while(i<5):\n",
        "  threshold_cv.append(i)\n",
        "  pred_y_cv = [0 if e > i else 1 for e in error_df.Reconstruction_error.values]\n",
        "  conf_matrix_cv = confusion_matrix(error_df.True_class, pred_y_cv)\n",
        "  #print(conf_matrix_cv)\n",
        "  print('threshold = ',i)\n",
        "  print('Accuracy = ', round((conf_matrix_cv[0][0]+conf_matrix_cv[1][1])/(conf_matrix_cv[0][1]+conf_matrix_cv[0][0]+conf_matrix_cv[1][0]+conf_matrix_cv[1][1])*100,2))\n",
        "  Recall_cv = round(conf_matrix_cv[0][0]/(conf_matrix_cv[0][1]+conf_matrix_cv[0][0])*100,2)\n",
        "  Precision_cv = round(conf_matrix_cv[0][0]/(conf_matrix_cv[1][0]+conf_matrix_cv[0][0])*100,2)\n",
        "  precision.append(Precision_cv)\n",
        "  recall.append(Recall_cv)\n",
        "  i+=0.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYffGNbHmnaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting graph between recall and precision based on various thresholds\n",
        "plt.plot(threshold_cv, precision, label=\"Precision\",linewidth=5)\n",
        "plt.plot(threshold_cv, recall, label=\"Recall\",linewidth=5)\n",
        "plt.title('Precision and recall for different threshold values')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Precision/Recall')\n",
        "plt.legend()\n",
        "#plt.savefig('recall&precisionVSthreshold.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceV_wu9Zmnb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Selecting threshold based on the graph\n",
        "best_threshold=1.1\n",
        "pred_y_test = [0 if e > best_threshold else 1 for e in error_df_test.Reconstruction_error.values]\n",
        "conf_matrix = confusion_matrix(error_df_test.True_class, pred_y_test)\n",
        "print(conf_matrix)\n",
        "\n",
        "print('Accuracy = ', round((conf_matrix[0][0]+conf_matrix[1][1])/(conf_matrix[0][1]+conf_matrix[0][0]+conf_matrix[1][0]+conf_matrix[1][1])*100,2))\n",
        "Recall = round(conf_matrix[0][0]/(conf_matrix[0][1]+conf_matrix[0][0])*100,2)\n",
        "Precision = round(conf_matrix[0][0]/(conf_matrix[1][0]+conf_matrix[0][0])*100,2)\n",
        "print('Recall = ', Recall)\n",
        "print('Precision = ', Precision)\n",
        "print('F1-Score = ',round((2*Precision*Recall)/(Precision+Recall),2))\n",
        "print('************************')\n",
        "\n",
        "Labels = [\"Attack\",\"Normal\"]\n",
        "plt.figure(figsize=(3, 3))\n",
        "sns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "#plt.savefig('confusion_matrix.png')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J4fsSR1mnfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3KuuPEAmQ1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}